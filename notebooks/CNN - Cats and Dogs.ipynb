{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat and dog image classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,      # Feature scaling. Each pixel is between 0 and 255, we convert them to be between 0 and 1\n",
    "    shear_range = 0.2,     # Shear mapping random skews the image.\n",
    "    zoom_range = 0.2,      # Range at which it will randomly zoom into a picture\n",
    "    horizontal_flip = True # Randomly flipping half of the images horizontally. Relevant when there are no assumptions ofm horizontal assymetry. (real-world photos)\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    '../data/training_set',\n",
    "    target_size = (64, 64), # Size the image will be converted to (64px by 64px)\n",
    "    batch_size = 32,        # Number of images in each batch (32)\n",
    "    class_mode = 'binary'   # Classification type, binary or categorical, in this case it is cat and dog (binary)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255 # Feature scaling. Each pixel is between 0 and 255, we convert them to be between 0 and 1\n",
    ") \n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    '../data/test_set',\n",
    "    target_size = (64, 64), # Size the image will be converted to (64px by 64px)\n",
    "    batch_size = 32,        # Number of images in each batch (32)\n",
    "    class_mode = 'binary'   # Classification type, binary or categorical, in this case it is cat and dog (binary)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential() # Stating the model as a sequence of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "cnn.add(                          # .add is adding a new layer to the model\n",
    "    tf.keras.layers.Conv2D(       # layers module. Conv2D class\n",
    "        filters = 32,             # Number of feature detectors you want to apply (3)\n",
    "        kernel_size = 3,          # Size of the feature detector. (3x3 in this case)\n",
    "        activation = 'relu',      # Activation function. Choosing ReLU\n",
    "        input_shape = [64, 64, 3] # Stating input shape (64px by 64px, 3 layers being RGB) (1 layer would be black and white)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.MaxPool2D( # Layers module. MaxPool2D class.\n",
    "        pool_size = 2,         # Width and height of square which scans the feature map (2)\n",
    "        strides = 2            # Distance which the square moves. Equal to pool size so same place isn't scanned twice.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters = 32,       # As above\n",
    "        kernel_size = 3,    # As above\n",
    "        activation = 'relu' # As above. Only need to state input_shape once on the first layer.\n",
    "    )\n",
    ")\n",
    "\n",
    "cnn.add(\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size = 2,      # As above\n",
    "        strides = 2         # As above\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.Flatten() # Flattens into a 1D vector.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.Dense(  # Dense class.\n",
    "        units = 128,        # Number of hidden neurons you want on the layer.\n",
    "        activation = 'relu' # Activation function. Chosen ReLU.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "cnn.add(\n",
    "    tf.keras.layers.Dense(     # Dense class.\n",
    "        units = 1,             # Number of neurons in output layer for our case is 1. Binary classification, need to know whether cat or dog.\n",
    "        activation = 'sigmoid' # Sigmoid is for binary classification. Softmax is for categorical classification.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "    optimizer = 'adam',           # Adam optimiser for stochastic gradient descent to adjust the weights.\n",
    "    loss = 'binary_crossentropy', # Binary cross entropy loss for binary classification.\n",
    "    metrics = ['accuracy']        # Accuracy metrics because it's the most relevent way to check performance of classification model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the training set and evaluating it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 130s 521ms/step - loss: 0.6590 - accuracy: 0.6059 - val_loss: 0.5827 - val_accuracy: 0.7110\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.5967 - accuracy: 0.6860 - val_loss: 0.6025 - val_accuracy: 0.6745\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.5523 - accuracy: 0.7201 - val_loss: 0.5637 - val_accuracy: 0.7225\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.5231 - accuracy: 0.7400 - val_loss: 0.5215 - val_accuracy: 0.7460\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.5108 - accuracy: 0.7473 - val_loss: 0.4912 - val_accuracy: 0.7630\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.4870 - accuracy: 0.7591 - val_loss: 0.5150 - val_accuracy: 0.7675\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.4653 - accuracy: 0.7735 - val_loss: 0.4903 - val_accuracy: 0.7795\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.4560 - accuracy: 0.7800 - val_loss: 0.4749 - val_accuracy: 0.7720\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.4445 - accuracy: 0.7881 - val_loss: 0.4637 - val_accuracy: 0.7950\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.4177 - accuracy: 0.8025 - val_loss: 0.4547 - val_accuracy: 0.7895\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.4047 - accuracy: 0.8101 - val_loss: 0.4800 - val_accuracy: 0.7870\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.3944 - accuracy: 0.8215 - val_loss: 0.4669 - val_accuracy: 0.7995\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 58s 234ms/step - loss: 0.3861 - accuracy: 0.8199 - val_loss: 0.4535 - val_accuracy: 0.8115\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3732 - accuracy: 0.8332 - val_loss: 0.4768 - val_accuracy: 0.7920\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.3638 - accuracy: 0.8367 - val_loss: 0.4533 - val_accuracy: 0.8105\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.3409 - accuracy: 0.8493 - val_loss: 0.4645 - val_accuracy: 0.8000\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.3261 - accuracy: 0.8526 - val_loss: 0.4561 - val_accuracy: 0.8120\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 60s 239ms/step - loss: 0.3235 - accuracy: 0.8572 - val_loss: 0.5229 - val_accuracy: 0.7890\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 0.3117 - accuracy: 0.8618 - val_loss: 0.5028 - val_accuracy: 0.7905\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.2974 - accuracy: 0.8709 - val_loss: 0.5250 - val_accuracy: 0.7905\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 56s 226ms/step - loss: 0.2761 - accuracy: 0.8881 - val_loss: 0.5296 - val_accuracy: 0.7935\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 0.2681 - accuracy: 0.8871 - val_loss: 0.5090 - val_accuracy: 0.7940\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 0.2515 - accuracy: 0.8921 - val_loss: 0.4802 - val_accuracy: 0.8055\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.2307 - accuracy: 0.9057 - val_loss: 0.5438 - val_accuracy: 0.8045\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.2176 - accuracy: 0.9135 - val_loss: 0.5090 - val_accuracy: 0.8075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2f95aacac8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(\n",
    "    x = training_set,           # Specifying the traning set\n",
    "    validation_data = test_set, # Specifying the test set\n",
    "    epochs = 25                 # Number of epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [],
   "source": [
    "test_image = image.load_img(\n",
    "    '../data/single_prediction/sacha.jpg',\n",
    "    target_size = (64, 64)                        # Specifying image size wanted to convert to as before.\n",
    ")\n",
    "\n",
    "test_image = image.img_to_array(test_image) # Converts into numpy array which is what is expected by next function.\n",
    "\n",
    "test_image = np.expand_dims( # We trained on batches of images. We have an extra dimension of the batch. Single image needs to be as a batch still.\n",
    "    test_image, # Image converted into a numpy array\n",
    "    axis = 0    # Add extra dimension, converting image to a batch with only one image in. We want the first [0] dimension\n",
    ")\n",
    "\n",
    "result = cnn.predict(test_image) # Predict the preprocessed image. result = [[1.]]\n",
    "\n",
    "training_set.class_indices # Get class indices from training set. {'cats': 0, 'dogs': 1}\n",
    "\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ED9KB3I54c1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ../models/cnn/cats_and_dogs/assets\n"
     ]
    }
   ],
   "source": [
    "cnn.save('../models/cnn/cats_and_dogs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2f6435f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = tf.keras.models.load_model(\"../models/cnn/cats_and_dogs\")\n",
    "\n",
    "# Let's check:\n",
    "outcome = np.testing.assert_allclose(\n",
    "    cnn.predict(test_image),\n",
    "    reconstructed_model.predict(test_image)\n",
    ")\n",
    "\n",
    "# Assertion error raised if records don't match. No errors, model loaded successfully."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
